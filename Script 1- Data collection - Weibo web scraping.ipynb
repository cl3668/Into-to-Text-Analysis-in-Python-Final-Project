{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdfbad9",
   "metadata": {},
   "source": [
    "# Project introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f80f9f45",
   "metadata": {},
   "source": [
    "China is one of the countries facing low birth rate issues, meanwhile, China's population decreased by 850,000 in 2022, marking the first negative growth in over 60 years despite years of work trying to avoid the situation. China repealed the One-Child policy in 2016 and launched birth-encouraging policies in earlier years. However, the birth rate has not increased as the government anticipated. (https://www.npr.org/2023/01/17/1149453055/china-records-1st-population-fall-in-decades-as-births-drop) <br>\n",
    "\n",
    "The topic of this project will understand the attitude toward the government's birth-encouraging policies online among the Chinese people. Intuitively, it is not hard to imagine that the general opinion may not be very supportive. Still, I am interested in the degree of dissatisfaction across time and what aspects people discussed most when commenting on the topic. To do so, I went through the process of data collection, data cleaning, and text analysis.  <br>\n",
    "\n",
    "For text analysis, I performed sentimental analysis to answer my question of people’s degree of satisfaction/ dissatisfaction from 2021-2023, used word clouds to demonstrate the overall most-frequent word, and used TF-IDF to identify year-specific keywords and further interpret possible reasons behind it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af28337",
   "metadata": {},
   "source": [
    "# Script 1: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d08b0c",
   "metadata": {},
   "source": [
    "## Main reference for this script\n",
    "The below code is adapted on script from this link https://zhuanlan.zhihu.com/p/443802888, there are a lot of changes as weibo changes their anti-crawling algorithen often. <br> Also, I also ask my friend Henry Hsieh for creating this script, thank him for his patience and kindness, this is his github: https://github.com/Hsieh-Cheng-Han"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da1bd5",
   "metadata": {},
   "source": [
    "## Decide what weibo comments to scrape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4055437d",
   "metadata": {},
   "source": [
    "The second step of the analysis is to decide the particular posts and their comments for analysis. For this step, I leverage a website (Link: https://weibo.zhaoyizhe.com/#) that recorded the most popular Weibo (ranked by view times in 24 hours) and searched birth-policy-related words to find the most popular posts and selected the ones with the most comments for web scraping, the URLs I selected to scrape as below:<br>\n",
    "1. https://m.weibo.cn/detail/4727660435213482\n",
    "2. https://m.weibo.cn/detail/4635622721458463\n",
    "3. https://m.weibo.cn/detail/4643035767642831\n",
    "4. https://m.weibo.cn/detail/4726596705715427\n",
    "5. https://m.weibo.cn/detail/4660999765890124\n",
    "6. https://m.weibo.cn/status/4635609970509108\n",
    "7. https://m.weibo.cn/detail/4660990257666817\n",
    "8. https://m.weibo.cn/status/4875327685790516\n",
    "9. https://m.weibo.cn/detail/4881790165059616\n",
    "10. https://m.weibo.cn/detail/4734289247207643\n",
    "11. https://m.weibo.cn/detail/4734283263247434\n",
    "12. https://m.weibo.cn/detail/4734299180368580<br>\n",
    "After selecting the posts, I create the script below to scrape the comments and save them as an Excel file to easily eyeball the results and perform preliminary data cleaning and combination through Excel. Then, I convert the Excel files into txt format through the below code and upload them to a folded name “Weibo by year” in jupyternotebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35680cd5",
   "metadata": {},
   "source": [
    "# Web scraping framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff95bcac",
   "metadata": {},
   "source": [
    "Unfortunately, the main data sources I wish to use for the analysis do not provide API for researchers, and hence required me to scrape data. Below is a simple framework to demonstrate the main steps of web scraping:\n",
    "1. Identify the data to be scraped: The first step is to decide what data you want to obtain. For this project, I selected two candidates and extracted the text only. \n",
    "2. Select a web scraping tool: The second step is to choose a web scraping tool. In class we used BeatuifulSoup as an example. However, due to the website's structure, I used Regular Expression (regex/re) to extract the content. \n",
    "3. Identify the website's structure: The third step is to investigate the website structure and source data to identify where the content I want is located. I performed this by leveraging Chrome’s developer tools to view the HTML and CSS> \n",
    "4. Write a web scraping script: The fourth step is to create a web scraping script based on my understanding from step 3. We did a similar thing with the Met website by using requests, parsing, etc., in class. \n",
    "5. Clean and store the data: Clean and process the extracted data to make it usable for your analysis or application. Store the data in a structured format, such as a CSV or database, for easy retrieval and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d72c77",
   "metadata": {},
   "source": [
    "# Determine the data source: Weibo vs Zhihu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff2c018f",
   "metadata": {},
   "source": [
    "To obtain the data representing the public’s opinion most, I selected Weibo (China’s Twitter) and Zhihu (China’s Quora) to compare and eventually used Weibo. The main difference between the two is that Weibo is quantitatively more advantageous in comments, while Zhihu is better in the quality of response. In other words, Weibo may contain more comments from more people, but it may be short lol. However, for Zhihu, there is like to have more discussions with keywords such as “not practical” and “should do XX instead of YYY.”<br>\n",
    "As I was unsure which data source would provide a better result, I scraped both data sources, ran tokenization and sentimental analysis to compare, and decided to proceed with Weibo as it has more in quantity and a more balanced amount of data across the three years. I will describe only the process of how I selected which Weibo to scrape below. If you are interested in the process of scaping Zhihu and the data quality check, please refer to the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e03c87",
   "metadata": {},
   "source": [
    "# Step1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from openpyxl import workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d964963",
   "metadata": {},
   "source": [
    "# Step2: Create a function to scrape Weibo comment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abb4307b",
   "metadata": {},
   "source": [
    "Weibo is known to be hard to scrape as its data is extremely valuable. I tried to battle with the website for a couple of days and then realized that the main trick: use the mobile url but not the web url. The mobile link is not well-protected as a web url. All I need to do is to provide my login info. However, there are also drawbacks to using mobile url to scrape. It is much slower in speed and often breaks down. I did my best to collect as many comments as my computer and time allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138009e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibo(mid):\n",
    "    url1 = \"https://m.weibo.cn/comments/hotflow\"\n",
    "    # root url\n",
    "    params1 = {\n",
    "        \"id\": mid,\n",
    "        \"mid\": mid,\n",
    "        \"max_id_type\": \"0\"\n",
    "    }\n",
    "    # set the request parameters\n",
    "    headers1 = {\n",
    "        'referer': 'https://m.weibo.cn/detail/4727660435213482?sudaref=m.weibo.cn',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',\n",
    "    }\n",
    "    # weibo requires you to log in to view the comment, so setting my log in info here\n",
    "    r1 = requests.get(url1, headers=headers1, params=params1).json()\n",
    "    # extract page infos\n",
    "    uid = r1['data']['data'][0]['analysis_extra']\n",
    "    uid = re.findall('\\d+', uid)[0]\n",
    "    max_id = '0'\n",
    "    url = \"https://weibo.com/ajax/statuses/buildComments\"\n",
    "    headers = {\n",
    "        'cookie': 'SINAGLOBAL=8515648282164.871.1666852129080; UOR=,,www.baidu.com; ULV=1680064186885:14:13:6:260922579667.9904.1680064186858:1679991894944; XSRF-TOKEN=4WYJ2HnYjeNQurJxrds3VP9x; SCF=AkTKZXmPlQFbWGbhqaBkHERup0-oJgr3GUuV1YeajJn_MdKUZyckI6tPmN76Eb7GeK_RWSzxzEHXyQ8PIKyLtVE.; SUB=_2A25JJ6J4DeRhGeFJ41sS9C7Ozz2IHXVqVJSwrDV8PUNbmtANLWH7kW9NfvbT3JC2MFhzX8uTCOBK8zDuqDtHtsMu; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9W5C6rxAfHsjpOHRQ6ImUVsu5JpX5KzhUgL.FoMN1h.0Sh5ESh22dJLoIERLxKnLBK2L12eLxKnL1h5L1h-LxK-L1-BLBKB_i--fi-82iK.7i--NiKyFiKnE; ALF=1711605160; SSOLoginState=1680069160; WBPSESS=Dt2hbAUaXfkVprjyrAZT_K7wd_BdTIfuEX6in29Oo1z_Mg08JjzTLAekKHQBMUQjbpWPgTbzu_3khRhOd61INlydGZoCGZaIhQRhJm5Nq1cTi4-A6ATPx9gnCqmBgcxQ8BRVeRT3jq_7KD9XgSNb7msAfU_BhIxkaNevuqiRcRtBvrZEcYur_HSDVKiKj_pRCCTdhVmz0afQw-tgFVKg6g==',\n",
    "        'referer': 'https://weibo.com/1642512402/MyjJCleeI?refer_flag=1001030103_',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',\n",
    "\n",
    "    }\n",
    "    # details of my weibo info, I used a window computer to do this as I don't want to log in weibo on my mac\n",
    "    dic = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "           'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "    p = 1\n",
    "    while 1:\n",
    "        li.append(max_id)\n",
    "        params = {\n",
    "            \"flow\": \"0\",\n",
    "            \"is_reload\": \"1\",\n",
    "            \"id\": mid,\n",
    "            \"is_show_bulletin\": \"2\",\n",
    "            \"is_mix\": \"0\",\n",
    "            \"max_id\": max_id,\n",
    "            \"count\": \"20\",\n",
    "            \"uid\": uid,\n",
    "            \"fetch_level\": \"0\"\n",
    "        }\n",
    "        if p == 1:\n",
    "            params = {\n",
    "                \"is_reload\": \"1\",\n",
    "                \"id\": mid,\n",
    "                \"is_show_bulletin\": \"2\",\n",
    "                \"is_mix\": \"0\",\n",
    "                \"count\": \"10\",\n",
    "                \"uid\": uid,\n",
    "                \"fetch_level\": \"0\"\n",
    "            }\n",
    "        r = requests.get(url, headers=headers, params=params).json()\n",
    "        # print(r)\n",
    "        max_id = r['max_id']\n",
    "        # print(max_id)\n",
    "        if max_id in li:\n",
    "            break\n",
    "        data = r['data']\n",
    "        for i in data:\n",
    "            text = i['text']\n",
    "            pattern1 = re.compile(r'<[^>]+>')  # match html tags\n",
    "            pattern2 = re.compile(r'[\\s+\\/_,$%^(MISSING)*(+]+|[+——￥%!…(MISSING)…&]+')  # match special symbols\n",
    "            text = pattern1.sub('', text)  # remove html tags\n",
    "            text = pattern2.sub('', text)  # remove special symbols\n",
    "            times = i['created_at'].split(' ')\n",
    "            date = times[-1] + '-' + dic[f'{times[1]}'] + '-' + times[2]\n",
    "            ws.append([text, date])\n",
    "            print(text)\n",
    "            print(date)\n",
    "        p += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe6720",
   "metadata": {},
   "source": [
    "# Step 3: Run the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    wb = workbook.Workbook()  # create an excel work book\n",
    "    ws = wb.active  # activate the weibo urls I want to scrape\n",
    "    ws.append(['评论', '日期'])  # append the text and date of the comments from the urls\n",
    "    li = []\n",
    "    mid = input('请输入uid:')\n",
    "    weibo(mid)\n",
    "    wb.save(f'微博{mid}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de7cb4",
   "metadata": {},
   "source": [
    "# Step 4: Manually check the excel sheets and aggregate into by-year files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92915db9",
   "metadata": {},
   "source": [
    "For this step, I opened all the excel files created from above to take a quick look of the data and manually categorized them by year. After converting into by-year fiele, I used the code in step 5 to convert them into txt for ease of text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd7bc4",
   "metadata": {},
   "source": [
    "# Step 5: Converting xlsx to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just a generic process of converting xlsx to txt for ease of analysis\n",
    "import pandas as pd\n",
    "xl = pd.ExcelFile('combined_weibo_comment.xlsx')\n",
    "\n",
    "for sheet in xl.sheet_names:\n",
    "    file = pd.read_excel(xl, sheet_name = sheet)\n",
    "    file.to_csv(sheet+'.txt', header = False, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
